{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import sys\n",
    "import numpy\n",
    "import numpy as np\n",
    "#import project_tests as tests\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, TimeDistributed, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous ADJ cars NOUN []\n",
      "cars NOUN shift VERB [Autonomous]\n",
      "shift VERB shift VERB [cars, liability]\n",
      "four NUM liability NOUN []\n",
      "insurance NOUN liability NOUN []\n",
      "liability NOUN shift VERB [four, insurance, toward]\n",
      "toward ADP liability NOUN [SFD]\n",
      "manufacturers NOUN SFD PROPN []\n",
      "SFD PROPN toward ADP [manufacturers]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"Autonomous cars shift four insurance liability toward manufacturers SFD\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.head.text, token.head.pos_,\n",
    "          [child for child in token.children])\n",
    "noun_key = {}\n",
    "reverse_noun_key = {}\n",
    "verb_key = {}\n",
    "reverse_verb_key = {}\n",
    "number_key = {}\n",
    "reverse_number_key = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "questions = open('fourth_grade_questions.txt','r')\n",
    "labels = open('fourth_grade_labels.txt','r')\n",
    "info_sentences = open('fourth_grade_info_sentences.txt','r')\n",
    "fact_sentences = open('fourth_grade_fact_sentences.txt','r')\n",
    "standard_relations = open('fourth_grade_standard_relations.txt','r')\n",
    "\n",
    "questions_list = []\n",
    "labels_list = []\n",
    "info_list = []\n",
    "relations_list = []\n",
    "facts_list = []\n",
    "\n",
    "for sentence in questions:\n",
    "    sentence = sentence.lower()\n",
    "    questions_list.append(sentence)\n",
    "    \n",
    "for sentence in labels:\n",
    "    labels_list.append(sentence)\n",
    "    \n",
    "for sentence in info_sentences:\n",
    "    sentence = sentence.lower()\n",
    "    info_list.append(sentence)\n",
    "\n",
    "for sentence in fact_sentences:\n",
    "    sentence = sentence.lower()\n",
    "    facts_list.append(sentence)\n",
    "    \n",
    "for sentence in standard_relations:\n",
    "    relations_list.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n",
      "16\n",
      "227\n",
      "customer service representatives is a compound word\n",
      "\n",
      "347\n"
     ]
    }
   ],
   "source": [
    "print(len(questions_list[0]))\n",
    "print(len(labels_list[0]))\n",
    "print(len(info_list[0]))\n",
    "print((facts_list[0]))\n",
    "print(len(relations_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Iterate over sentences and use Spacy\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    " \n",
    "pad_index = 999999\n",
    "num_index = 0\n",
    "adj_index = 100000\n",
    "verb_index = 200000\n",
    "noun_index = 300000\n",
    "punct_index = 400000\n",
    "det_index = 500000\n",
    "propn_index = 500000\n",
    "adv_index = 600000\n",
    "conj_index = 700000\n",
    "else_index = 800000\n",
    "rela_index = 900000\n",
    "#etc etc\n",
    "\n",
    "new_facts_list =[]\n",
    "new_questions_list = []\n",
    "new_info_list = []\n",
    "new_relations_list = []\n",
    "\n",
    "pad_up_to = 50\n",
    "\n",
    "for sentence in facts_list:\n",
    "    substitute_sentence = [] #replace each word with index\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.pos_== \"NUM\": # if word is a num\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                #Number index is special. It is equal to the number itself\n",
    "                #We will convert all numbers written in numbers to numbers\n",
    "                if token.text == \"one\":\n",
    "                    num_index = 1\n",
    "                elif token.text == \"two\":\n",
    "                    num_index = 2\n",
    "                elif token.text == \"three\":\n",
    "                    num_index = 3\n",
    "                elif token.text == \"four\":\n",
    "                    num_index = 4\n",
    "                elif token.text == \"five\":\n",
    "                    num_index = 5\n",
    "                elif token.text == \"six\":\n",
    "                    num_index = 6\n",
    "                elif token.text == \"seven\":\n",
    "                    num_index = 7\n",
    "                elif token.text == \"eight\":\n",
    "                    num_index = 8\n",
    "                elif token.text == \"nine\":\n",
    "                    num_index = 9\n",
    "                elif token.text == \"ten\":\n",
    "                    num_index = 10\n",
    "                #We will stop at 10 numbers. For numeric numbers, we will simply convert to int. We can only go upto 99999.\n",
    "                else:\n",
    "                    try:\n",
    "                        num_index = int(token.text)\n",
    "                    except:\n",
    "                        num_index = 99999 #If number is not recognized, just make up an index.\n",
    "                word_to_index[(token.text,token.pos_)]= num_index\n",
    "                index_to_word[num_index] = (token.text, token.pos_)\n",
    "                #convert sentence to indices\n",
    "                substitute_sentence.append(num_index)\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "        \n",
    "        if token.pos_== \"ADJ\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= adj_index\n",
    "                index_to_word[adj_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(adj_index)\n",
    "                adj_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"VERB\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= verb_index\n",
    "                index_to_word[verb_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(verb_index)\n",
    "                verb_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"NOUN\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= noun_index\n",
    "                index_to_word[noun_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(noun_index)\n",
    "                noun_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"DET\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= det_index\n",
    "                index_to_word[det_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(det_index)\n",
    "                det_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"PROPN\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= propn_index\n",
    "                index_to_word[propn_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(propn_index)\n",
    "                propn_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"PUNCT\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= punct_index\n",
    "                index_to_word[punct_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(punct_index)\n",
    "                punct_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"ADV\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= adv_index\n",
    "                index_to_word[adv_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(adv_index)\n",
    "                adv_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"CONJ\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= conj_index\n",
    "                index_to_word[conj_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(conj_index)\n",
    "                conj_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        else: #anything else, dump in else_index\n",
    "            if (token.text,\"else\") not in word_to_index.keys():\n",
    "                word_to_index[(token.text,\"else\")]= else_index\n",
    "                index_to_word[else_index] = (token.text, \"else\")\n",
    "                substitute_sentence.append(else_index)\n",
    "                else_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,\"else\")])\n",
    "    \n",
    "    #pad each and then add to the facts_list\n",
    "    for i in range(len(substitute_sentence),pad_up_to):\n",
    "        substitute_sentence.append(pad_index)\n",
    "    new_facts_list.append(substitute_sentence)\n",
    "    \n",
    "for sentence in info_list:\n",
    "    substitute_sentence = [] #replace each word with index\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.pos_== \"NUM\": # if word is a num\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                #Number index is special. It is equal to the number itself\n",
    "                #We will convert all numbers written in numbers to numbers\n",
    "                if token.text == \"one\":\n",
    "                    num_index = 1\n",
    "                elif token.text == \"two\":\n",
    "                    num_index = 2\n",
    "                elif token.text == \"three\":\n",
    "                    num_index = 3\n",
    "                elif token.text == \"four\":\n",
    "                    num_index = 4\n",
    "                elif token.text == \"five\":\n",
    "                    num_index = 5\n",
    "                elif token.text == \"six\":\n",
    "                    num_index = 6\n",
    "                elif token.text == \"seven\":\n",
    "                    num_index = 7\n",
    "                elif token.text == \"eight\":\n",
    "                    num_index = 8\n",
    "                elif token.text == \"nine\":\n",
    "                    num_index = 9\n",
    "                elif token.text == \"ten\":\n",
    "                    num_index = 10\n",
    "                #We will stop at 10 numbers. For numeric numbers, we will simply convert to int. We can only go upto 99999.\n",
    "                else:\n",
    "                    try:\n",
    "                        num_index = int(token.text)\n",
    "                    except:\n",
    "                        num_index = 99999 #If number is not recognized, just make up an index.\n",
    "                word_to_index[(token.text,token.pos_)]= num_index\n",
    "                index_to_word[num_index] = (token.text, token.pos_)\n",
    "                #convert sentence to indices\n",
    "                substitute_sentence.append(num_index)\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "        \n",
    "        if token.pos_== \"ADJ\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= adj_index\n",
    "                index_to_word[adj_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(adj_index)\n",
    "                adj_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"VERB\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= verb_index\n",
    "                index_to_word[verb_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(verb_index)\n",
    "                verb_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"NOUN\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= noun_index\n",
    "                index_to_word[noun_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(noun_index)\n",
    "                noun_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"DET\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= det_index\n",
    "                index_to_word[det_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(det_index)\n",
    "                det_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"PROPN\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= propn_index\n",
    "                index_to_word[propn_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(propn_index)\n",
    "                propn_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"PUNCT\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= punct_index\n",
    "                index_to_word[punct_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(punct_index)\n",
    "                punct_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"ADV\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= adv_index\n",
    "                index_to_word[adv_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(adv_index)\n",
    "                adv_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"CONJ\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= conj_index\n",
    "                index_to_word[conj_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(conj_index)\n",
    "                conj_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        else: #anything else, dump in else_index\n",
    "            if (token.text,\"else\") not in word_to_index.keys():\n",
    "                word_to_index[(token.text,\"else\")]= else_index\n",
    "                index_to_word[else_index] = (token.text, \"else\")\n",
    "                substitute_sentence.append(else_index)\n",
    "                else_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,\"else\")])\n",
    "\n",
    "    #pad each and then add to the facts_list\n",
    "    for i in range(len(substitute_sentence),pad_up_to*2): #info sentences are long, so we just make them double length\n",
    "        substitute_sentence.append(pad_index)\n",
    "    new_info_list.append(substitute_sentence)\n",
    "    \n",
    "for sentence in questions_list:\n",
    "    substitute_sentence = [] #replace each word with index\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.pos_== \"NUM\": # if word is a num\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                #Number index is special. It is equal to the number itself\n",
    "                #We will convert all numbers written in numbers to numbers\n",
    "                if token.text == \"one\":\n",
    "                    num_index = 1\n",
    "                elif token.text == \"two\":\n",
    "                    num_index = 2\n",
    "                elif token.text == \"three\":\n",
    "                    num_index = 3\n",
    "                elif token.text == \"four\":\n",
    "                    num_index = 4\n",
    "                elif token.text == \"five\":\n",
    "                    num_index = 5\n",
    "                elif token.text == \"six\":\n",
    "                    num_index = 6\n",
    "                elif token.text == \"seven\":\n",
    "                    num_index = 7\n",
    "                elif token.text == \"eight\":\n",
    "                    num_index = 8\n",
    "                elif token.text == \"nine\":\n",
    "                    num_index = 9\n",
    "                elif token.text == \"ten\":\n",
    "                    num_index = 10\n",
    "                #We will stop at 10 numbers. For numeric numbers, we will simply convert to int. We can only go upto 99999.\n",
    "                else:\n",
    "                    try:\n",
    "                        num_index = int(token.text)\n",
    "                    except:\n",
    "                        num_index = 99999 #If number is not recognized, just make up an index.\n",
    "                word_to_index[(token.text,token.pos_)]= num_index\n",
    "                index_to_word[num_index] = (token.text, token.pos_)\n",
    "                #convert sentence to indices\n",
    "                substitute_sentence.append(num_index)\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "        \n",
    "        if token.pos_== \"ADJ\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= adj_index\n",
    "                index_to_word[adj_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(adj_index)\n",
    "                adj_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"VERB\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= verb_index\n",
    "                index_to_word[verb_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(verb_index)\n",
    "                verb_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"NOUN\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= noun_index\n",
    "                index_to_word[noun_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(noun_index)\n",
    "                noun_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"DET\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= det_index\n",
    "                index_to_word[det_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(det_index)\n",
    "                det_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"PROPN\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= propn_index\n",
    "                index_to_word[propn_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(propn_index)\n",
    "                propn_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"PUNCT\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= punct_index\n",
    "                index_to_word[punct_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(punct_index)\n",
    "                punct_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"ADV\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= adv_index\n",
    "                index_to_word[adv_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(adv_index)\n",
    "                adv_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"CONJ\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= conj_index\n",
    "                index_to_word[conj_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(conj_index)\n",
    "                conj_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        else: #anything else, dump in else_index\n",
    "            if (token.text,\"else\") not in word_to_index.keys():\n",
    "                word_to_index[(token.text,\"else\")]= else_index\n",
    "                index_to_word[else_index] = (token.text, \"else\")\n",
    "                substitute_sentence.append(else_index)\n",
    "                else_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,\"else\")])\n",
    "\n",
    "    #pad each and then add to the facts_list\n",
    "    for i in range(len(substitute_sentence),pad_up_to):\n",
    "        substitute_sentence.append(pad_index)\n",
    "    new_questions_list.append(substitute_sentence)\n",
    "    \n",
    "\n",
    "#For relations, we will use the hand-typed file for this example. For future studies, a function can easily be coded\n",
    "#Since we hand-wrote the file, we made it easy to process by including tuple. Now we can use the word_to_index\n",
    "for sentence in relations_list:\n",
    "    substitute_sentence = []\n",
    "    sent = sentence.split()\n",
    "    for char in sent: #char could be a word or group of characters\n",
    "        if (char, \"NOUN\") in word_to_index.keys():\n",
    "            substitute_sentence.append(word_to_index[(char,\"NOUN\")])\n",
    "        if (char, \"VERB\") in word_to_index.keys():\n",
    "            substitute_sentence.append(word_to_index[(char,\"VERB\")])\n",
    "        if (char, \"ADJ\") in word_to_index.keys():\n",
    "            substitute_sentence.append(word_to_index[(char,\"ADJ\")])\n",
    "        if (char, \"rela\") not in word_to_index.keys():\n",
    "            word_to_index[(char, \"rela\")] = rela_index\n",
    "            substitute_sentence.append(rela_index)\n",
    "            rela_index += 1\n",
    "        else: \n",
    "            substitute_sentence.append(word_to_index[(char,\"rela\")])\n",
    "    \n",
    "    for i in range(len(substitute_sentence),pad_up_to*2): #relation sentences are the longest, so we double the padding\n",
    "        substitute_sentence.append(pad_index)\n",
    "    new_relations_list.append(substitute_sentence)\n",
    "\n",
    "index_to_words[999999] = (\"<PAD>\",\"pad\")\n",
    "words_to_index[(\"<PAD>\",\"pad\")] = 999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(new_facts_list[0]))\n",
    "print(len(new_questions_list[0]))\n",
    "print(len(new_info_list[0]))\n",
    "print(len(new_relations_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def vectorize_sentences(sentence_list):\n",
    "    new_list = []\n",
    "    for sentence in sentence_list:\n",
    "        vectorized_sentence = []\n",
    "        for index in sentence:\n",
    "            vector = []\n",
    "            for dgit in str(index):\n",
    "                vector.append(int(dgit))\n",
    "            vectorozeros = [0]*(6-len(vector))\n",
    "            vectorozeros.extend(vector)\n",
    "            vector = vectorozeros\n",
    "            vectorized_sentence.append(vector)\n",
    "        new_list.append(vectorized_sentence)\n",
    "    return new_list\n",
    "#check\n",
    "\n",
    "vectorized_fact_list = np.asarray(vectorize_sentences(new_facts_list))\n",
    "vectorized_questions = np.asarray(vectorize_sentences(new_questions_list)) \n",
    "vectorized_info = np.asarray(vectorize_sentences(new_info_list))\n",
    "vectorized_relations = np.asarray(vectorize_sentences(new_relations_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 50, 6)\n",
      "(34, 50, 6)\n",
      "(34, 100, 6)\n",
      "(34, 100, 6)\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_fact_list.shape)\n",
    "print(vectorized_questions.shape)\n",
    "print(vectorized_info.shape)\n",
    "print(vectorized_relations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "label_words_to_index = {}\n",
    "label_index_to_words = {}\n",
    "label_index = 1\n",
    "label_pad_index = 0\n",
    "new_label_list = []\n",
    "for sentence in labels_list:\n",
    "    substitute_sentence = []\n",
    "    sent = sentence.split()\n",
    "    for char in sent: #char could be a word or group of characters\n",
    "        if char in label_words_to_index.keys():\n",
    "            substitute_sentence.append(label_words_to_index[char])\n",
    "        else:\n",
    "            label_words_to_index[char]=label_index\n",
    "            label_index_to_words[label_index]=char\n",
    "            substitute_sentence.append(label_words_to_index[char])\n",
    "            label_index += 1\n",
    "    for i in range(len(substitute_sentence),pad_up_to): #pad to regular\n",
    "        substitute_sentence.append(label_pad_index)\n",
    "    new_label_list.append(substitute_sentence)\n",
    "    \n",
    "label_array = np.asarray(new_label_list)\n",
    "new_label_array = label_array.reshape(*new_label_list.shape, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "print(new_label_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, dictionary):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"    \n",
    "    return ' '.join([label_index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cascaded_reasoning(info_shape, questions_shape, facts_shape \\\n",
    "                       , relations_shape, labels_index_len):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    learning_rate = 0.01\n",
    "    hidden_units = 128\n",
    "    small_dense = 128 \n",
    "    large_dense = 256\n",
    "    # TODO: Build the layers\n",
    "    \n",
    "    LSTM_info_inputs = Input(shape=(None, info_shape[1], info_shape[2]))\n",
    "    LSTM_info = LSTM(hidden_units,return_sequences=True)\n",
    "    LSTM_info_output=LSTM_info(LSTM_info_inputs)\n",
    "    LSTM_info_dense = Dense(small_dense,activation=\"tanh\")\n",
    "    LSTM_info_dense_outputs = LSTM_info_dense(LSTM_info_output)\n",
    "    \n",
    "    LSTM_questions_inputs = Input(shape=(None, questions_shape[1], questions_shape[2]))\n",
    "    LSTM_questions = LSTM(hidden_units,return_sequences=True)\n",
    "    LSTM_questions_output=LSTM_questions(LSTM_questions_inputs)\n",
    "    LSTM_questions_dense = Dense(small_dense,activation=\"tanh\")\n",
    "    LSTM_questions_dense_outputs = LSTM_questions_dense(LSTM_questions_output)\n",
    "\n",
    "    model_info = Model([LSTM_info_inputs], LSTM_info_dense_outputs)\n",
    "    model_ques = Model([LSTM_questions_inputs], LSTM_questions_dense_outputs)\n",
    "    \n",
    "    #model_A = Sequential()\n",
    "    #model_A.add(Merge([model_info,model_ques],mode='concat'))\n",
    "    \n",
    "    #model_B = Sequential()\n",
    "    #model_B.add(Merge([model_facts,model_rela],mode='concat'))\n",
    "    \n",
    "    #model_T = Sequential()\n",
    "    #model_T.add(Merge([model_A,model_B],mode='concat'))\n",
    "    #model_T.add(Dense(large_dense))\n",
    "    #model_T.add(Dense(labels_index_len))\n",
    "\n",
    "    #model_T.compile(loss=sparse_categorical_crossentropy,\n",
    "     #             optimizer=Adam(learning_rate),\n",
    "      #            metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#Example inputs: X1 = \"There are four apples at the top of the tree.\" X2 = \"What do you need to get the apples?\"\n",
    "# X3 = \"Trees are high.\"  Label result = \"Trees -> High\"\n",
    "#Example inputs: X1 = \"There are four apples at the top of the tree.\" X2 = \"What do you need to get the apples?\"\n",
    "# X3 = \"Pick apples from tree.\"  Label result = \"Apples -> On tree\"\n",
    "#Example inputs: X1 = \"Apples->On tree, Tree->High\" X2 = \"What do you need to get the apples?\"\n",
    "# X3 = \"You can go on tree with a ladder.\"  Label result = \"Ladder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add() got an unexpected keyword argument 'input_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-9dc23cf5387b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#tests.test_model_final(model_final)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcascaded_reasoning\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcascaded_reasoning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorized_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorized_questions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m                        \u001b[0mvectorized_fact_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorized_relations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_index_to_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cascaded_reasoning loaded'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-97-133939fe22cc>\u001b[0m in \u001b[0;36mcascaded_reasoning\u001b[1;34m(info_shape, questions_shape, facts_shape, relations_shape, labels_index_len)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# TODO: Build the layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mmodel_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mmodel_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGRU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minfo_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mmodel_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmall_dense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m  \u001b[1;33m=\u001b[0m  \u001b[1;34m\"tanh\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: add() got an unexpected keyword argument 'input_shape'"
     ]
    }
   ],
   "source": [
    "#tests.test_model_final(model_final)\n",
    "\n",
    "cascaded_reasoning= cascaded_reasoning(vectorized_info.shape, vectorized_questions.shape, \\\n",
    "                       vectorized_fact_list.shape, vectorized_relations.shape, len(label_index_to_words))\n",
    "\n",
    "print('cascaded_reasoning loaded')\n",
    "\n",
    "cascaded_reasoning.fit([[vectorized_info, vectorized_questions], \\\n",
    "                        [vectorized_fact_list, vectorized_relations]], new_label_array, batch_size=34, epochs=10, validation_split=0.1)\n",
    "\n",
    "print(logits_to_text(cascaded_reasoning.predict([[vectorized_info[:1], new_questions.list[:1]], \\\n",
    "                        [new_facts_list[:1], new_relations_list[:1]]], label_index_to_words)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Train the final model\n",
    "\n",
    "#Example test:There are 4 doctors working in a clinic. Each doctor has 2 nurses assisting them. There\n",
    "#are two receptionists, Jay and Molly, working at the reception.\n",
    "#On Monday, 23 patients made appointments with each doctor. However, 6 of the\n",
    "#patients did not show up. How many patients visited the clinic on Monday?\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence = \"There are 4 doctors working in a clinic. There are 4 doctors working in a clinic. \\\n",
    "          Each doctor has 2 nurses assisting them. There are two receptionists, Jay and Molly, \\\n",
    "          working at the reception. On Monday, 23 patients made appointments with each doctor. \\\n",
    "          However, 6 of the patients did not show up.\"\n",
    "question = \"How many patients visited the clinic on Monday?\"\n",
    "doc = nlp(sentence)\n",
    "indx_n = 0\n",
    "indx_v = 0\n",
    "indx_num = 0\n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        indx_n += 1\n",
    "        relations.append(\"N%d = %s\".format(indx_n, token.text))\n",
    "        noun_key[token.text] = \"N%d\".format(indx_n)\n",
    "        reverse_noun_key[\"N%d\".format(indx_n)]=token.text\n",
    "    if token.pos_ == \"VERB\":\n",
    "        indx_v += 1\n",
    "        relations.append(\"V%d = %s\".format(indx_v, token.text))\n",
    "        verb_key[token.text] = \"V%d\".format(indx_v)\n",
    "        reverse_verb_key[\"V%d\".format(indx_v)]=token.text\n",
    "    if token.dep_ == \"num\" or token.dep_ == \"num\": #this is deprecated\n",
    "        indx_num += 1\n",
    "        relations.append(\"NUM%d = %s\".format(indx_num, token.text))\n",
    "        number_key[token.text] = \"NUM%d\".format(indx_num)\n",
    "        reverse_number_key[\"NUM%d\".format(indx_num)]=token.text\n",
    "\n",
    "#We will not convert the numbers. Instead, we will use vector representations of the numbers. The machine can figure \n",
    "#out the relation between the numbers in the sentence. We will represent every word with a 10-digit vector. The \n",
    "#first digits will indicate the type of word; for example: representative can be represented by the vector 9000000602,  \n",
    "#where 9 is for noun. Another example: work = 6000000052, where 6 is for verb. Now, for numbers, there will be zeros in the \n",
    "#beginning. For example, 3 will simply be 0000000003, and 24 = 0000000024. We will let the network figure out that these \n",
    "#represent actual numbers.   \n",
    "\n",
    "#create variables to be used in assignments:\n",
    "var_list = []\n",
    "for indx_var in range(10):\n",
    "    var_list[indx_var+1] = \"VAR%d\".format(indx_var+1) #Add 1 so that VAR list starts with VAR1\n",
    "    \n",
    "fact_list[0] = \"A physician, medical practitioner, medical doctor, or simply doctor is a professional who practises medicine, \\\n",
    "which is concerned with promoting, maintaining, or restoring health through the study, diagnosis, and treatment \\\n",
    "of disease, injury, and other physical and mental impairments.\" \n",
    "\n",
    "#Now collect info about each of the nouns based on the question:\n",
    "for noun in noun_list:\n",
    "    #example: noun being focussed = doctors \n",
    "    for fact in fact_list:  \n",
    "        inputs = [sentence, question, relations, facts]\n",
    "        #we expect an output that will yield a sort of relation between the words that will be useful to answer the question:\n",
    "        #these relations will be initially input by humans to train the network on what to look for\n",
    "        #in this case, the output could be a list of four or five relations, each relation composed of up to four words: \n",
    "        #[doctor->practices->medicine, medicine->about->health, 23 patients->made->appointments, 4->doctors]\n",
    "        #The output from this iteration will be ammended to the relations list and fed into the next iteration.\n",
    "        #It is possible to have the network find the relations itself by stacking several of these networks. However, the\n",
    "        #resulting mega-network will be too huge to train. A supercomputer may be necessary.   \n",
    "        #For now, we will fix the question and info, and iterate over the fact sentences. Labels will correspond to facts. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
