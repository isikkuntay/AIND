{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import sys\n",
    "import numpy\n",
    "import numpy as np\n",
    "#import project_tests as tests\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, TimeDistributed, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Merge\n",
    "from keras.layers import Lambda\n",
    "from keras import backend as K\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous ADJ cars NOUN []\n",
      "cars NOUN shift VERB [Autonomous]\n",
      "shift VERB shift VERB [cars, liability]\n",
      "four NUM liability NOUN []\n",
      "insurance NOUN liability NOUN []\n",
      "liability NOUN shift VERB [four, insurance, toward]\n",
      "toward ADP liability NOUN [SFD]\n",
      "manufacturers NOUN SFD PROPN []\n",
      "SFD PROPN toward ADP [manufacturers]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"Autonomous cars shift four insurance liability toward manufacturers SFD\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.head.text, token.head.pos_,\n",
    "          [child for child in token.children])\n",
    "noun_key = {}\n",
    "reverse_noun_key = {}\n",
    "verb_key = {}\n",
    "reverse_verb_key = {}\n",
    "number_key = {}\n",
    "reverse_number_key = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "questions = open('fourth_grade_questions.txt','r')\n",
    "labels = open('fourth_grade_labels.txt','r')\n",
    "info_sentences = open('fourth_grade_info_sentences.txt','r')\n",
    "fact_sentences = open('fourth_grade_fact_sentences.txt','r')\n",
    "standard_relations = open('fourth_grade_standard_relations.txt','r')\n",
    "\n",
    "questions_list = []\n",
    "labels_list = []\n",
    "info_list = []\n",
    "relations_list = []\n",
    "facts_list = []\n",
    "\n",
    "for sentence in questions:\n",
    "    sentence = sentence.lower()\n",
    "    questions_list.append(sentence)\n",
    "    \n",
    "for sentence in labels:\n",
    "    labels_list.append(sentence)\n",
    "    \n",
    "for sentence in info_sentences:\n",
    "    sentence = sentence.lower()\n",
    "    info_list.append(sentence)\n",
    "\n",
    "for sentence in fact_sentences:\n",
    "    sentence = sentence.lower()\n",
    "    facts_list.append(sentence)\n",
    "    \n",
    "for sentence in standard_relations:\n",
    "    relations_list.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n",
      "16\n",
      "227\n",
      "customer service representatives is a compound word\n",
      "\n",
      "347\n"
     ]
    }
   ],
   "source": [
    "print(len(questions_list[0]))\n",
    "print(len(labels_list[0]))\n",
    "print(len(info_list[0]))\n",
    "print((facts_list[0]))\n",
    "print(len(relations_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Iterate over sentences and use Spacy\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    " \n",
    "pad_index = 999999\n",
    "num_index = 0\n",
    "adj_index = 100000\n",
    "verb_index = 200000\n",
    "noun_index = 300000\n",
    "punct_index = 400000\n",
    "det_index = 500000\n",
    "propn_index = 500000\n",
    "adv_index = 600000\n",
    "conj_index = 700000\n",
    "else_index = 800000\n",
    "rela_index = 900000\n",
    "#etc etc\n",
    "\n",
    "new_facts_list =[]\n",
    "new_questions_list = []\n",
    "new_info_list = []\n",
    "new_relations_list = []\n",
    "\n",
    "pad_up_to = 290\n",
    "\n",
    "for sentence in facts_list:\n",
    "    substitute_sentence = [] #replace each word with index\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.pos_== \"NUM\": # if word is a num\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                #Number index is special. It is equal to the number itself\n",
    "                #We will convert all numbers written in numbers to numbers\n",
    "                if token.text == \"one\":\n",
    "                    num_index = 1\n",
    "                elif token.text == \"two\":\n",
    "                    num_index = 2\n",
    "                elif token.text == \"three\":\n",
    "                    num_index = 3\n",
    "                elif token.text == \"four\":\n",
    "                    num_index = 4\n",
    "                elif token.text == \"five\":\n",
    "                    num_index = 5\n",
    "                elif token.text == \"six\":\n",
    "                    num_index = 6\n",
    "                elif token.text == \"seven\":\n",
    "                    num_index = 7\n",
    "                elif token.text == \"eight\":\n",
    "                    num_index = 8\n",
    "                elif token.text == \"nine\":\n",
    "                    num_index = 9\n",
    "                elif token.text == \"ten\":\n",
    "                    num_index = 10\n",
    "                #We will stop at 10 numbers. For numeric numbers, we will simply convert to int. We can only go upto 99999.\n",
    "                else:\n",
    "                    try:\n",
    "                        num_index = int(token.text)\n",
    "                    except:\n",
    "                        num_index = 99999 #If number is not recognized, just make up an index.\n",
    "                word_to_index[(token.text,token.pos_)]= num_index\n",
    "                index_to_word[num_index] = (token.text, token.pos_)\n",
    "                #convert sentence to indices\n",
    "                substitute_sentence.append(num_index)\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "        \n",
    "        if token.pos_== \"ADJ\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= adj_index\n",
    "                index_to_word[adj_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(adj_index)\n",
    "                adj_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"VERB\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= verb_index\n",
    "                index_to_word[verb_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(verb_index)\n",
    "                verb_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"NOUN\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= noun_index\n",
    "                index_to_word[noun_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(noun_index)\n",
    "                noun_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"DET\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= det_index\n",
    "                index_to_word[det_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(det_index)\n",
    "                det_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"PROPN\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= propn_index\n",
    "                index_to_word[propn_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(propn_index)\n",
    "                propn_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"PUNCT\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= punct_index\n",
    "                index_to_word[punct_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(punct_index)\n",
    "                punct_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"ADV\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= adv_index\n",
    "                index_to_word[adv_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(adv_index)\n",
    "                adv_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"CONJ\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= conj_index\n",
    "                index_to_word[conj_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(conj_index)\n",
    "                conj_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        else: #anything else, dump in else_index\n",
    "            if (token.text,\"else\") not in word_to_index.keys():\n",
    "                word_to_index[(token.text,\"else\")]= else_index\n",
    "                index_to_word[else_index] = (token.text, \"else\")\n",
    "                substitute_sentence.append(else_index)\n",
    "                else_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,\"else\")])\n",
    "    \n",
    "    #pad each and then add to the facts_list\n",
    "    for i in range(len(substitute_sentence),pad_up_to):\n",
    "        substitute_sentence.append(pad_index)\n",
    "    new_facts_list.append(substitute_sentence)\n",
    "    \n",
    "for sentence in info_list:\n",
    "    substitute_sentence = [] #replace each word with index\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.pos_== \"NUM\": # if word is a num\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                #Number index is special. It is equal to the number itself\n",
    "                #We will convert all numbers written in numbers to numbers\n",
    "                if token.text == \"one\":\n",
    "                    num_index = 1\n",
    "                elif token.text == \"two\":\n",
    "                    num_index = 2\n",
    "                elif token.text == \"three\":\n",
    "                    num_index = 3\n",
    "                elif token.text == \"four\":\n",
    "                    num_index = 4\n",
    "                elif token.text == \"five\":\n",
    "                    num_index = 5\n",
    "                elif token.text == \"six\":\n",
    "                    num_index = 6\n",
    "                elif token.text == \"seven\":\n",
    "                    num_index = 7\n",
    "                elif token.text == \"eight\":\n",
    "                    num_index = 8\n",
    "                elif token.text == \"nine\":\n",
    "                    num_index = 9\n",
    "                elif token.text == \"ten\":\n",
    "                    num_index = 10\n",
    "                #We will stop at 10 numbers. For numeric numbers, we will simply convert to int. We can only go upto 99999.\n",
    "                else:\n",
    "                    try:\n",
    "                        num_index = int(token.text)\n",
    "                    except:\n",
    "                        num_index = 99999 #If number is not recognized, just make up an index.\n",
    "                word_to_index[(token.text,token.pos_)]= num_index\n",
    "                index_to_word[num_index] = (token.text, token.pos_)\n",
    "                #convert sentence to indices\n",
    "                substitute_sentence.append(num_index)\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "        \n",
    "        if token.pos_== \"ADJ\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= adj_index\n",
    "                index_to_word[adj_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(adj_index)\n",
    "                adj_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"VERB\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= verb_index\n",
    "                index_to_word[verb_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(verb_index)\n",
    "                verb_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"NOUN\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= noun_index\n",
    "                index_to_word[noun_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(noun_index)\n",
    "                noun_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"DET\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= det_index\n",
    "                index_to_word[det_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(det_index)\n",
    "                det_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"PROPN\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= propn_index\n",
    "                index_to_word[propn_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(propn_index)\n",
    "                propn_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"PUNCT\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= punct_index\n",
    "                index_to_word[punct_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(punct_index)\n",
    "                punct_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"ADV\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= adv_index\n",
    "                index_to_word[adv_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(adv_index)\n",
    "                adv_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"CONJ\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= conj_index\n",
    "                index_to_word[conj_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(conj_index)\n",
    "                conj_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        else: #anything else, dump in else_index\n",
    "            if (token.text,\"else\") not in word_to_index.keys():\n",
    "                word_to_index[(token.text,\"else\")]= else_index\n",
    "                index_to_word[else_index] = (token.text, \"else\")\n",
    "                substitute_sentence.append(else_index)\n",
    "                else_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,\"else\")])\n",
    "\n",
    "    #pad each and then add to the facts_list\n",
    "    for i in range(len(substitute_sentence),pad_up_to): \n",
    "        substitute_sentence.append(pad_index)\n",
    "    new_info_list.append(substitute_sentence)\n",
    "    \n",
    "for sentence in questions_list:\n",
    "    substitute_sentence = [] #replace each word with index\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.pos_== \"NUM\": # if word is a num\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                #Number index is special. It is equal to the number itself\n",
    "                #We will convert all numbers written in numbers to numbers\n",
    "                if token.text == \"one\":\n",
    "                    num_index = 1\n",
    "                elif token.text == \"two\":\n",
    "                    num_index = 2\n",
    "                elif token.text == \"three\":\n",
    "                    num_index = 3\n",
    "                elif token.text == \"four\":\n",
    "                    num_index = 4\n",
    "                elif token.text == \"five\":\n",
    "                    num_index = 5\n",
    "                elif token.text == \"six\":\n",
    "                    num_index = 6\n",
    "                elif token.text == \"seven\":\n",
    "                    num_index = 7\n",
    "                elif token.text == \"eight\":\n",
    "                    num_index = 8\n",
    "                elif token.text == \"nine\":\n",
    "                    num_index = 9\n",
    "                elif token.text == \"ten\":\n",
    "                    num_index = 10\n",
    "                #We will stop at 10 numbers. For numeric numbers, we will simply convert to int. We can only go upto 99999.\n",
    "                else:\n",
    "                    try:\n",
    "                        num_index = int(token.text)\n",
    "                    except:\n",
    "                        num_index = 99999 #If number is not recognized, just make up an index.\n",
    "                word_to_index[(token.text,token.pos_)]= num_index\n",
    "                index_to_word[num_index] = (token.text, token.pos_)\n",
    "                #convert sentence to indices\n",
    "                substitute_sentence.append(num_index)\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "        \n",
    "        if token.pos_== \"ADJ\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= adj_index\n",
    "                index_to_word[adj_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(adj_index)\n",
    "                adj_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"VERB\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= verb_index\n",
    "                index_to_word[verb_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(verb_index)\n",
    "                verb_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"NOUN\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= noun_index\n",
    "                index_to_word[noun_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(noun_index)\n",
    "                noun_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"DET\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= det_index\n",
    "                index_to_word[det_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(det_index)\n",
    "                det_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"PROPN\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= propn_index\n",
    "                index_to_word[propn_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(propn_index)\n",
    "                propn_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"PUNCT\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= punct_index\n",
    "                index_to_word[punct_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(punct_index)\n",
    "                punct_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"ADV\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= adv_index\n",
    "                index_to_word[adv_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(adv_index)\n",
    "                adv_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        if token.pos_== \"CONJ\": # if word is a verb\n",
    "            if (token.text,token.pos_) not in word_to_index.keys():\n",
    "                word_to_index[(token.text,token.pos_)]= conj_index\n",
    "                index_to_word[conj_index] = (token.text, token.pos_)\n",
    "                substitute_sentence.append(conj_index)\n",
    "                conj_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,token.pos_)])\n",
    "\n",
    "        else: #anything else, dump in else_index\n",
    "            if (token.text,\"else\") not in word_to_index.keys():\n",
    "                word_to_index[(token.text,\"else\")]= else_index\n",
    "                index_to_word[else_index] = (token.text, \"else\")\n",
    "                substitute_sentence.append(else_index)\n",
    "                else_index += 1\n",
    "            else: #if word already in the dictionary, we still add the index to the substitute sentence\n",
    "                substitute_sentence.append(word_to_index[(token.text,\"else\")])\n",
    "\n",
    "    #pad each and then add to the facts_list\n",
    "    for i in range(len(substitute_sentence),pad_up_to):\n",
    "        substitute_sentence.append(pad_index)\n",
    "    new_questions_list.append(substitute_sentence)\n",
    "    \n",
    "\n",
    "#For relations, we will use the hand-typed file for this example. For future studies, a function can easily be coded\n",
    "#Since we hand-wrote the file, we made it easy to process by including tuple. Now we can use the word_to_index\n",
    "for sentence in relations_list:\n",
    "    substitute_sentence = []\n",
    "    sent = sentence.split()\n",
    "    for char in sent: #char could be a word or group of characters\n",
    "        if (char, \"NOUN\") in word_to_index.keys():\n",
    "            substitute_sentence.append(word_to_index[(char,\"NOUN\")])\n",
    "        if (char, \"VERB\") in word_to_index.keys():\n",
    "            substitute_sentence.append(word_to_index[(char,\"VERB\")])\n",
    "        if (char, \"ADJ\") in word_to_index.keys():\n",
    "            substitute_sentence.append(word_to_index[(char,\"ADJ\")])\n",
    "        if (char, \"rela\") not in word_to_index.keys():\n",
    "            word_to_index[(char, \"rela\")] = rela_index\n",
    "            substitute_sentence.append(rela_index)\n",
    "            rela_index += 1\n",
    "        else: \n",
    "            substitute_sentence.append(word_to_index[(char,\"rela\")])\n",
    "    \n",
    "    for i in range(len(substitute_sentence),pad_up_to): \n",
    "        substitute_sentence.append(pad_index)\n",
    "    new_relations_list.append(substitute_sentence)\n",
    "\n",
    "word_to_index[(\"<PAD>\",\"pad\")] = 999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290\n",
      "290\n",
      "290\n",
      "290\n"
     ]
    }
   ],
   "source": [
    "print(len(new_facts_list[0]))\n",
    "print(len(new_questions_list[0]))\n",
    "print(len(new_info_list[0]))\n",
    "print(len(new_relations_list[33]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def vectorize_sentences(sentence_list):\n",
    "    new_list = []\n",
    "    for sentence in sentence_list:\n",
    "        vectorized_sentence = []\n",
    "        for index in sentence:\n",
    "            vector = []\n",
    "            for dgit in str(index):\n",
    "                vector.append(int(dgit))\n",
    "            vectorozeros = [0]*(6-len(vector))\n",
    "            vectorozeros.extend(vector)\n",
    "            vector = vectorozeros\n",
    "            vectorized_sentence.append(vector)\n",
    "        new_list.append(vectorized_sentence)\n",
    "    return new_list\n",
    "#check\n",
    "\n",
    "vectorized_fact_list = np.asarray(vectorize_sentences(new_facts_list))\n",
    "vectorized_questions = np.asarray(vectorize_sentences(new_questions_list)) \n",
    "vectorized_info = np.asarray(vectorize_sentences(new_info_list))\n",
    "vectorized_relations = np.asarray(vectorize_sentences(new_relations_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 290, 6)\n",
      "(34, 290, 6)\n",
      "(34, 290, 6)\n",
      "(34, 290, 6)\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_fact_list.shape)\n",
    "print(vectorized_questions.shape)\n",
    "print(vectorized_info.shape)\n",
    "print(vectorized_relations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "label_words_to_index = {}\n",
    "label_index_to_words = {}\n",
    "label_index = 0\n",
    "label_words_to_index[\"\\t\"]=label_index\n",
    "label_index_to_words[label_index]=\"\\t\"\n",
    "label_index += 1\n",
    "label_words_to_index[\"<PAD>\"]=label_index\n",
    "label_index_to_words[label_index]=\"<PAD>\"\n",
    "new_label_list = []\n",
    "label_pad_up_to = 50\n",
    "for sentence in labels_list:\n",
    "    substitute_sentence = []\n",
    "    sent = sentence.split()\n",
    "    for char in sent: #char could be a word or group of characters\n",
    "        if char in label_words_to_index.keys():\n",
    "            substitute_sentence.append(label_words_to_index[char])\n",
    "        else:\n",
    "            label_index += 1\n",
    "            label_words_to_index[char]=label_index\n",
    "            label_index_to_words[label_index]=char\n",
    "            substitute_sentence.append(label_words_to_index[char])\n",
    "    for i in range(len(substitute_sentence),label_pad_up_to): #pad to regular\n",
    "        substitute_sentence.append(label_words_to_index[\"<PAD>\"])\n",
    "        \n",
    "    new_label_list.append(substitute_sentence)\n",
    "    \n",
    "#vectorize labels\n",
    "vectorized_label_list = []\n",
    "for sentence in new_label_list:\n",
    "    sentence_vector = []\n",
    "    for word in sentence:\n",
    "        vector = [0]*len(label_words_to_index)\n",
    "        vector[word] = 1\n",
    "        sentence_vector.append(vector)\n",
    "    vectorized_label_list.append(sentence_vector)\n",
    "    \n",
    "label_array = np.asarray(vectorized_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 50, 48)\n",
      "48\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "print(label_array.shape)\n",
    "print(len(label_words_to_index))\n",
    "print(len(label_index_to_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, dictionary):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"    \n",
    "    return ' '.join([label_index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cascaded_reasoning(info_shape, questions_shape, facts_shape \\\n",
    "                       , relations_shape, labels_index_len):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    learning_rate = 0.01\n",
    "    hidden_units = 128\n",
    "    latent_dim = 128 \n",
    "    # TODO: Build the layers\n",
    "    \n",
    "    LSTM_info_inputs = Input(shape=(None, info_shape[2]))\n",
    "    LSTM_info = LSTM(hidden_units,return_state=True)\n",
    "    LSTM_info_output, info_h, info_c=LSTM_info(LSTM_info_inputs)\n",
    "    info_states = [info_h,info_c]\n",
    "    \n",
    "    LSTM_questions_inputs = Input(shape=(None, questions_shape[2]))\n",
    "    LSTM_questions = LSTM(hidden_units,return_state=True)\n",
    "    LSTM_questions_output, questions_h, questions_c=LSTM_questions(LSTM_questions_inputs)\n",
    "    questions_states= [questions_h,questions_c]\n",
    "    \n",
    "    LSTM_facts_inputs = Input(shape=(None, facts_shape[2]))\n",
    "    LSTM_facts = LSTM(hidden_units,return_state=True)\n",
    "    LSTM_facts_output, facts_h, facts_c=LSTM_facts(LSTM_facts_inputs)\n",
    "    facts_states= [facts_h,facts_c]\n",
    "    \n",
    "    LSTM_rela_inputs = Input(shape=(None, relations_shape[2]))\n",
    "    LSTM_rela = LSTM(hidden_units,return_state=True)\n",
    "    LSTM_rela_output,rela_h,rela_c=LSTM_rela(LSTM_rela_inputs)\n",
    "    rela_states= [rela_h,rela_c]\n",
    "    \n",
    "    added_h = keras.layers.Add()([info_h,questions_h,facts_h,rela_h])\n",
    "    added_c = keras.layers.Add()([info_c,questions_c,facts_c,rela_c])\n",
    "    states = [added_h,added_c]\n",
    "    \n",
    "    decoder_inputs = Input(shape=(1, labels_index_len))\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_dense = Dense(labels_index_len, activation='softmax')\n",
    "\n",
    "    all_outputs = []\n",
    "    inputs = decoder_inputs\n",
    "    for _ in range(label_pad_up_to):\n",
    "        # Run the decoder on one timestep\n",
    "        outputs, state_h, state_c = decoder_lstm(inputs,\n",
    "                                             initial_state=states)\n",
    "        outputs = decoder_dense(outputs)\n",
    "        # Store the current prediction (we will concatenate all predictions later)\n",
    "        all_outputs.append(outputs)\n",
    "        # Reinject the outputs as inputs for the next loop iteration\n",
    "        # as well as update the states\n",
    "        inputs = outputs\n",
    "        states = [state_h, state_c]\n",
    "\n",
    "    # Concatenate all predictions\n",
    "    decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n",
    "\n",
    "    # Define and compile model as previously\n",
    "    model_T = Model([LSTM_info_inputs, LSTM_questions_inputs, LSTM_facts_inputs, \\\n",
    "                     LSTM_rela_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    model_T.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model_T\n",
    "\n",
    "#Example inputs: X1 = \"There are four apples at the top of the tree.\" X2 = \"What do you need to get the apples?\"\n",
    "# X3 = \"Trees are high.\"  Label result = \"Trees -> High\"\n",
    "#Example inputs: X1 = \"There are four apples at the top of the tree.\" X2 = \"What do you need to get the apples?\"\n",
    "# X3 = \"Pick apples from tree.\"  Label result = \"Apples -> On tree\"\n",
    "#Example inputs: X1 = \"Apples->On tree, Tree->High\" X2 = \"What do you need to get the apples?\"\n",
    "# X3 = \"You can go on tree with a ladder.\"  Label result = \"Ladder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "cascaded_reasoning loaded\n",
      "Train on 30 samples, validate on 4 samples\n",
      "Epoch 1/10\n",
      "30/30 [==============================] - 551s 18s/step - loss: 3.8827 - acc: 0.0033 - val_loss: 3.5070 - val_acc: 0.7800\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 4s 126ms/step - loss: 3.2308 - acc: 0.9080 - val_loss: 2.9829 - val_acc: 0.7800\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 3s 95ms/step - loss: 2.5316 - acc: 0.9080 - val_loss: 1.8108 - val_acc: 0.7800\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 3s 92ms/step - loss: 1.2660 - acc: 0.9080 - val_loss: 1.2264 - val_acc: 0.7800\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 3s 92ms/step - loss: 0.5790 - acc: 0.9080 - val_loss: 1.1702 - val_acc: 0.7800\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 3s 92ms/step - loss: 0.5011 - acc: 0.9080 - val_loss: 1.1783 - val_acc: 0.7800\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 3s 95ms/step - loss: 0.4686 - acc: 0.9080 - val_loss: 1.1581 - val_acc: 0.7800\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 3s 92ms/step - loss: 0.4486 - acc: 0.9080 - val_loss: 1.1859 - val_acc: 0.7800\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 3s 91ms/step - loss: 0.4353 - acc: 0.9080 - val_loss: 1.1281 - val_acc: 0.7800\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 3s 92ms/step - loss: 0.4255 - acc: 0.9080 - val_loss: 1.2050 - val_acc: 0.7800\n",
      "model fit\n"
     ]
    }
   ],
   "source": [
    "#tests.test_model_final(model_final)\n",
    "batch_size = 34\n",
    "\n",
    "print(len(label_index_to_words))\n",
    "\n",
    "cascaded_reasoning= cascaded_reasoning(vectorized_info.shape, vectorized_questions.shape, \\\n",
    "                       vectorized_fact_list.shape, vectorized_relations.shape, len(label_index_to_words))\n",
    "\n",
    "print('cascaded_reasoning loaded')\n",
    "\n",
    "# Prepare decoder input data that just contains the start character\n",
    "# Note that we could have made it a constant hard-coded in the model\n",
    "decoder_input_data = np.zeros((batch_size, 1, len(label_index_to_words)))\n",
    "decoder_input_data[:, 0, label_words_to_index['\\t']] = 1.\n",
    "\n",
    "cascaded_reasoning.fit([vectorized_info, vectorized_questions, \\\n",
    "                        vectorized_fact_list, vectorized_relations, decoder_input_data], \\\n",
    "                       label_array, batch_size=batch_size, epochs=10, validation_split=0.1)\n",
    "\n",
    "print(\"model fit\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-39b9b8965b22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits_to_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcascaded_reasoning\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvectorized_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorized_questions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m                         \u001b[1;33m[\u001b[0m\u001b[0mvectorized_fact_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorized_relations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_index_to_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\envs\\aind-dl\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1815\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[0;32m   1816\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1817\u001b[1;33m                                     check_batch_axis=False)\n\u001b[0m\u001b[0;32m   1818\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1819\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\aind-dl\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\aind-dl\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "print(logits_to_text(cascaded_reasoning.predict([[vectorized_info[:1], vectorized_questions[:1]], \\\n",
    "                        [vectorized_fact_list[:1], vectorized_relations[:1]]], label_index_to_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Train the final model\n",
    "\n",
    "#Example test:There are 4 doctors working in a clinic. Each doctor has 2 nurses assisting them. There\n",
    "#are two receptionists, Jay and Molly, working at the reception.\n",
    "#On Monday, 23 patients made appointments with each doctor. However, 6 of the\n",
    "#patients did not show up. How many patients visited the clinic on Monday?\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence = \"There are 4 doctors working in a clinic. There are 4 doctors working in a clinic. \\\n",
    "          Each doctor has 2 nurses assisting them. There are two receptionists, Jay and Molly, \\\n",
    "          working at the reception. On Monday, 23 patients made appointments with each doctor. \\\n",
    "          However, 6 of the patients did not show up.\"\n",
    "question = \"How many patients visited the clinic on Monday?\"\n",
    "doc = nlp(sentence)\n",
    "indx_n = 0\n",
    "indx_v = 0\n",
    "indx_num = 0\n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        indx_n += 1\n",
    "        relations.append(\"N%d = %s\".format(indx_n, token.text))\n",
    "        noun_key[token.text] = \"N%d\".format(indx_n)\n",
    "        reverse_noun_key[\"N%d\".format(indx_n)]=token.text\n",
    "    if token.pos_ == \"VERB\":\n",
    "        indx_v += 1\n",
    "        relations.append(\"V%d = %s\".format(indx_v, token.text))\n",
    "        verb_key[token.text] = \"V%d\".format(indx_v)\n",
    "        reverse_verb_key[\"V%d\".format(indx_v)]=token.text\n",
    "    if token.dep_ == \"num\" or token.dep_ == \"num\": #this is deprecated\n",
    "        indx_num += 1\n",
    "        relations.append(\"NUM%d = %s\".format(indx_num, token.text))\n",
    "        number_key[token.text] = \"NUM%d\".format(indx_num)\n",
    "        reverse_number_key[\"NUM%d\".format(indx_num)]=token.text\n",
    "\n",
    "#We will not convert the numbers. Instead, we will use vector representations of the numbers. The machine can figure \n",
    "#out the relation between the numbers in the sentence. We will represent every word with a 10-digit vector. The \n",
    "#first digits will indicate the type of word; for example: representative can be represented by the vector 9000000602,  \n",
    "#where 9 is for noun. Another example: work = 6000000052, where 6 is for verb. Now, for numbers, there will be zeros in the \n",
    "#beginning. For example, 3 will simply be 0000000003, and 24 = 0000000024. We will let the network figure out that these \n",
    "#represent actual numbers.   \n",
    "\n",
    "#create variables to be used in assignments:\n",
    "var_list = []\n",
    "for indx_var in range(10):\n",
    "    var_list[indx_var+1] = \"VAR%d\".format(indx_var+1) #Add 1 so that VAR list starts with VAR1\n",
    "    \n",
    "fact_list[0] = \"A physician, medical practitioner, medical doctor, or simply doctor is a professional who practises medicine, \\\n",
    "which is concerned with promoting, maintaining, or restoring health through the study, diagnosis, and treatment \\\n",
    "of disease, injury, and other physical and mental impairments.\" \n",
    "\n",
    "#Now collect info about each of the nouns based on the question:\n",
    "for noun in noun_list:\n",
    "    #example: noun being focussed = doctors \n",
    "    for fact in fact_list:  \n",
    "        inputs = [sentence, question, relations, facts]\n",
    "        #we expect an output that will yield a sort of relation between the words that will be useful to answer the question:\n",
    "        #these relations will be initially input by humans to train the network on what to look for\n",
    "        #in this case, the output could be a list of four or five relations, each relation composed of up to four words: \n",
    "        #[doctor->practices->medicine, medicine->about->health, 23 patients->made->appointments, 4->doctors]\n",
    "        #The output from this iteration will be ammended to the relations list and fed into the next iteration.\n",
    "        #It is possible to have the network find the relations itself by stacking several of these networks. However, the\n",
    "        #resulting mega-network will be too huge to train. A supercomputer may be necessary.   \n",
    "        #For now, we will fix the question and info, and iterate over the fact sentences. Labels will correspond to facts. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
