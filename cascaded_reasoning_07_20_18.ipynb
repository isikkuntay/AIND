{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import sys\n",
    "import numpy\n",
    "import numpy as np\n",
    "#import project_tests as tests\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, TimeDistributed, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Merge\n",
    "from keras.layers import Lambda\n",
    "from keras import backend as K\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next ADJ are VERB [to]\n",
      "to ADP next ADJ [N1]\n",
      "the DET N1 NOUN []\n",
      "3 NUM N1 NOUN []\n",
      "N1 NOUN to ADP [the, 3]\n",
      ", PUNCT are VERB []\n",
      "there ADV are VERB []\n",
      "are VERB are VERB [next, ,, there, N2, .]\n",
      "5 NUM N2 PROPN []\n",
      "N2 PROPN are VERB [5]\n",
      ". PUNCT are VERB []\n",
      "on ADP are VERB [side]\n",
      "the DET side NOUN []\n",
      "other ADJ side NOUN []\n",
      "side NOUN on ADP [the, other, of]\n",
      "of ADP side NOUN [N3]\n",
      "the DET N3 NOUN []\n",
      "N3 NOUN of ADP [the]\n",
      ", PUNCT are VERB []\n",
      "there ADV are VERB []\n",
      "are VERB are VERB [on, ,, there, N2, .]\n",
      "7 NUM N2 PROPN []\n",
      "N2 PROPN are VERB [7]\n",
      ". PUNCT are VERB []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"next to the 3 N1 , there are 5 N2 . on the other side of the N3 , there are 7 N2 .\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.head.text, token.head.pos_,\n",
    "          [child for child in token.children])\n",
    "noun_key = {}\n",
    "reverse_noun_key = {}\n",
    "verb_key = {}\n",
    "reverse_verb_key = {}\n",
    "number_key = {}\n",
    "reverse_number_key = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "questions = open('abstract_math_questions.txt','r')\n",
    "labels = open('abstract_math_labels.txt','r')\n",
    "info_sentences = open('abstract_math_info.txt','r')\n",
    "fact_sentences = open('abstract_math_facts.txt','r')\n",
    "standard_relations = open('abstract_math_relations.txt','r')\n",
    "\n",
    "questions_list = []\n",
    "labels_list = []\n",
    "info_list = []\n",
    "relations_list = []\n",
    "facts_list = []\n",
    "\n",
    "for sentence in questions:\n",
    "    sentence = sentence.lower()\n",
    "    questions_list.append(sentence)\n",
    "    \n",
    "for sentence in labels:\n",
    "    labels_list.append(sentence)\n",
    "    \n",
    "for sentence in info_sentences:\n",
    "    sentence = sentence.lower()\n",
    "    info_list.append(sentence)\n",
    "\n",
    "for sentence in fact_sentences:\n",
    "    sentence = sentence.lower()\n",
    "    facts_list.append(sentence)\n",
    "    \n",
    "for sentence in standard_relations:\n",
    "    relations_list.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "10\n",
      "26\n",
      "5\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(questions_list[0]))\n",
    "print(len(labels_list[0]))\n",
    "print(len(info_list[0]))\n",
    "print(len(facts_list[0]))\n",
    "print(len(relations_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-81eafc2ea8f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfacts_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mfact_num_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mindx_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "#in this version, we put placeholders for numbers, such as QN1, QN2, etc.. for question numbers\n",
    "\n",
    "fact_num_dict_list = []\n",
    "ques_num_dict_list = []\n",
    "info_num_dict_list = []\n",
    "\n",
    "#store the numbers in a list of dicts\n",
    "#for facts list:\n",
    "for sentence in facts_list:\n",
    "    fact_num_dict = {}\n",
    "    indx_num = 1\n",
    "    for word in sentence:\n",
    "         if word.isdigit():\n",
    "            fact_num_dict[word] = \"FN%d\".format(indx_num)\n",
    "            word = \"FN%d\".format(indx_num)\n",
    "            indx_num += 1\n",
    "    fact_num_dict_list.append(fact_num_dict)\n",
    "\n",
    "#repeat for questions\n",
    "for sentence in questions_list:\n",
    "    quest_num_dict = {}\n",
    "    indx_num = 1\n",
    "    for word in sentence:\n",
    "         if word.isdigit():\n",
    "            quest_num_dict[word] = \"QN%d\".format(indx_num)\n",
    "            word = \"QN%d\".format(indx_num)\n",
    "            indx_num += 1\n",
    "    ques_num_dict_list.append(quest_num_dict)   \n",
    "\n",
    "#repeat for information\n",
    "for sentence in info_list:\n",
    "    info_num_dict = {}\n",
    "    indx_num = 1\n",
    "    for word in sentence:\n",
    "         if word.isdigit():\n",
    "            info_num_dict[word] = \"IN%d\".format(indx_num)\n",
    "            word = \"IN%d\".format(indx_num)\n",
    "            indx_num += 1\n",
    "    info_num_dict_list.append(info_num_dict)   \n",
    "\n",
    "#relations do not need since it will already be in encoded format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "list_of_all_variables = []\n",
    "\n",
    "### Let's make a list of all variables and prepare them for one-hot-coding\n",
    "\n",
    "# Start by assigning numbers to the standard variables:\n",
    "\n",
    "list_of_all_variables.append[\"QN1\"]\n",
    "list_of_all_variables.append[\"QN2\"]\n",
    "list_of_all_variables.append[\"QN3\"]\n",
    "list_of_all_variables.append[\"QN4\"]\n",
    "list_of_all_variables.append[\"QN5\"]\n",
    "list_of_all_variables.append[\"QN6\"]\n",
    "list_of_all_variables.append[\"QN7\"]\n",
    "list_of_all_variables.append[\"QN8\"]\n",
    "list_of_all_variables.append[\"FN1\"]\n",
    "list_of_all_variables.append[\"FN2\"]\n",
    "list_of_all_variables.append[\"FN3\"]\n",
    "list_of_all_variables.append[\"FN4\"]\n",
    "list_of_all_variables.append[\"FN5\"]\n",
    "list_of_all_variables.append[\"FN6\"]\n",
    "list_of_all_variables.append[\"FN7\"]\n",
    "list_of_all_variables.append[\"FN8\"]\n",
    "list_of_all_variables.append[\"IN1\"]\n",
    "list_of_all_variables.append[\"IN2\"]\n",
    "list_of_all_variables.append[\"IN3\"]\n",
    "list_of_all_variables.append[\"IN4\"]\n",
    "list_of_all_variables.append[\"IN5\"]\n",
    "list_of_all_variables.append[\"IN6\"]\n",
    "list_of_all_variables.append[\"IN7\"]\n",
    "list_of_all_variables.append[\"IN8\"]\n",
    "list_of_all_variables.append[\"N1\"]\n",
    "list_of_all_variables.append[\"N2\"]\n",
    "list_of_all_variables.append[\"N3\"]\n",
    "list_of_all_variables.append[\"N4\"]\n",
    "list_of_all_variables.append[\"N5\"]\n",
    "list_of_all_variables.append[\"N6\"]\n",
    "list_of_all_variables.append[\"N7\"]\n",
    "list_of_all_variables.append[\"N8\"]\n",
    "list_of_all_variables.append[\"N9\"]\n",
    "list_of_all_variables.append[\"V1\"]\n",
    "list_of_all_variables.append[\"V2\"]\n",
    "list_of_all_variables.append[\"V3\"]\n",
    "list_of_all_variables.append[\"V4\"]\n",
    "list_of_all_variables.append[\"V5\"]\n",
    "list_of_all_variables.append[\"V6\"]\n",
    "list_of_all_variables.append[\"V7\"]\n",
    "list_of_all_variables.append[\"V8\"]\n",
    "list_of_all_variables.append[\"V9\"]\n",
    "\n",
    "for sentence in facts_list:\n",
    "    for word in sentence:\n",
    "        if word not in list_of_all_variables:\n",
    "            list_of_all_variables.append()\n",
    "    \n",
    "for sentence in info_list:\n",
    "    for word in sentence:\n",
    "        if word not in list_of_all_variables:\n",
    "            list_of_all_variables.append()\n",
    "\n",
    "for sentence in questions_list:\n",
    "    for word in sentence:\n",
    "        if word not in list_of_all_variables:\n",
    "            list_of_all_variables.append()\n",
    "\n",
    "for sentence in relations_list:\n",
    "    for word in sentence:\n",
    "        if word not in list_of_all_variables:\n",
    "            list_of_all_variables.append()      \n",
    "            \n",
    "list_of_all_variables.append[\"PAD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "\n",
    "word_index = 0\n",
    "\n",
    "for word in list_of_all_variables:\n",
    "    word_to_index[word] = word_index\n",
    "    index_to_word[word_index] = word\n",
    "    word_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_facts_list = []\n",
    "new_questions_list = []\n",
    "new_info_list = []\n",
    "new_relations_list = []\n",
    "\n",
    "pad_up_to = 100\n",
    "\n",
    "for sentence in facts_list:\n",
    "    substitute_sentence = []\n",
    "    for word in sentence:\n",
    "        substitute_sentence.append(word_to_index[word])\n",
    "    for i in range(len(substitute_sentence),pad_up_to):\n",
    "        substitute_sentence.append(pad_index)\n",
    "        new_facts_list.append(substitute_sentence)\n",
    "\n",
    "for sentence in questions_list:\n",
    "    substitute_sentence = []\n",
    "    for word in sentence:\n",
    "        substitute_sentence.append(word_to_index[word])\n",
    "    for i in range(len(substitute_sentence),pad_up_to):\n",
    "        substitute_sentence.append(pad_index)\n",
    "        new_questions_list.append(substitute_sentence)\n",
    "        \n",
    "for sentence in info_list:\n",
    "    substitute_sentence = []\n",
    "    for word in sentence:\n",
    "        substitute_sentence.append(word_to_index[word])\n",
    "    for i in range(len(substitute_sentence),pad_up_to):\n",
    "        substitute_sentence.append(pad_index)\n",
    "        new_info_list.append(substitute_sentence)\n",
    "\n",
    "for sentence in relations_list:\n",
    "    substitute_sentence = []\n",
    "    for word in sentence:\n",
    "        substitute_sentence.append(word_to_index[word])\n",
    "    for i in range(len(substitute_sentence),pad_up_to):\n",
    "        substitute_sentence.append(pad_index)\n",
    "        new_relations_list.append(substitute_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(new_facts_list[0]))\n",
    "print(len(new_questions_list[0]))\n",
    "print(len(new_info_list[0]))\n",
    "print(len(new_relations_list[33]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "one_hot_facts = []\n",
    "one_hot_question = []\n",
    "one_hot_info = []\n",
    "one_hot_relations = []\n",
    "\n",
    "one_hot_length = len(word_to_index)\n",
    "\n",
    "for sentence in new_facts_list:\n",
    "    substitute_sentence = []\n",
    "    for word_ind in sentence:\n",
    "        for i in range(one_hot_length):\n",
    "            if i == word_ind:\n",
    "                substitute_sentence.append[1]\n",
    "            else:\n",
    "                substitute_sentence.append[0]\n",
    "    one_hot_facts.append(substitute_sentence)\n",
    "\n",
    "for sentence in new_questions_list:\n",
    "    substitute_sentence = []\n",
    "    for word_ind in sentence:\n",
    "        for i in range(one_hot_length):\n",
    "            if i == word_ind:\n",
    "                substitute_sentence.append[1]\n",
    "            else:\n",
    "                substitute_sentence.append[0]\n",
    "    one_hot_questions.append(substitute_sentence)\n",
    "\n",
    "for sentence in new_info_list:\n",
    "    substitute_sentence = []\n",
    "    for word_ind in sentence:\n",
    "        for i in range(one_hot_length):\n",
    "            if i == word_ind:\n",
    "                substitute_sentence.append[1]\n",
    "            else:\n",
    "                substitute_sentence.append[0]\n",
    "    one_hot_info.append(substitute_sentence)\n",
    "    \n",
    "for sentence in new_relations_list:\n",
    "    substitute_sentence = []\n",
    "    for word_ind in sentence:\n",
    "        for i in range(one_hot_length):\n",
    "            if i == word_ind:\n",
    "                substitute_sentence.append[1]\n",
    "            else:\n",
    "                substitute_sentence.append[0]\n",
    "    one_hot_relations.append(substitute_sentence)\n",
    "    \n",
    "vectorized_fact_list = np.asarray(one_hot_facts)\n",
    "vectorized_questions = np.asarray(one_hot_questions) \n",
    "vectorized_info = np.asarray(one_hot_info)\n",
    "vectorized_relations = np.asarray(one_hot_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 100, 6)\n",
      "(34, 100, 6)\n",
      "(34, 100, 6)\n",
      "(34, 300, 6)\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_fact_list.shape)\n",
    "print(vectorized_questions.shape)\n",
    "print(vectorized_info.shape)\n",
    "print(vectorized_relations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "label_words_to_index = {}\n",
    "label_index_to_words = {}\n",
    "label_index = 0\n",
    "label_words_to_index[\"\\t\"]=label_index\n",
    "label_index_to_words[label_index]=\"\\t\"\n",
    "label_index += 1\n",
    "label_words_to_index[\"<PAD>\"]=label_index\n",
    "label_index_to_words[label_index]=\"<PAD>\"\n",
    "new_label_list = []\n",
    "label_pad_up_to = 50\n",
    "for sentence in labels_list:\n",
    "    substitute_sentence = []\n",
    "    sent = sentence.split()\n",
    "    for char in sent: #char could be a word or group of characters\n",
    "        if char in label_words_to_index.keys():\n",
    "            substitute_sentence.append(label_words_to_index[char])\n",
    "        else:\n",
    "            label_index += 1\n",
    "            label_words_to_index[char]=label_index\n",
    "            label_index_to_words[label_index]=char\n",
    "            substitute_sentence.append(label_words_to_index[char])\n",
    "    for i in range(len(substitute_sentence),label_pad_up_to): #pad to regular\n",
    "        substitute_sentence.append(label_words_to_index[\"<PAD>\"])\n",
    "        \n",
    "    new_label_list.append(substitute_sentence)\n",
    "    \n",
    "#vectorize labels\n",
    "vectorized_label_list = []\n",
    "for sentence in new_label_list:\n",
    "    sentence_vector = []\n",
    "    for word in sentence:\n",
    "        vector = [0]*len(label_words_to_index)\n",
    "        vector[word] = 1\n",
    "        sentence_vector.append(vector)\n",
    "    vectorized_label_list.append(sentence_vector)\n",
    "    \n",
    "label_array = np.asarray(vectorized_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 50, 48)\n",
      "48\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "print(label_array.shape)\n",
    "print(len(label_words_to_index))\n",
    "print(len(label_index_to_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, dictionary):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"    \n",
    "    return ' '.join([label_index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cascaded_reasoning(info_shape, questions_shape, facts_shape \\\n",
    "                       , relations_shape, labels_index_len):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    learning_rate = 0.01\n",
    "    hidden_units = 128\n",
    "    latent_dim = 128 \n",
    "    # TODO: Build the layers\n",
    "    \n",
    "    LSTM_info_inputs = Input(shape=(None, info_shape[2]))\n",
    "    LSTM_info = LSTM(hidden_units,return_state=True)\n",
    "    LSTM_info_output, info_h, info_c=LSTM_info(LSTM_info_inputs)\n",
    "    info_states = [info_h,info_c]\n",
    "    \n",
    "    LSTM_questions_inputs = Input(shape=(None, questions_shape[2]))\n",
    "    LSTM_questions = LSTM(hidden_units,return_state=True)\n",
    "    LSTM_questions_output, questions_h, questions_c=LSTM_questions(LSTM_questions_inputs)\n",
    "    questions_states= [questions_h,questions_c]\n",
    "    \n",
    "    LSTM_facts_inputs = Input(shape=(None, facts_shape[2]))\n",
    "    LSTM_facts = LSTM(hidden_units,return_state=True)\n",
    "    LSTM_facts_output, facts_h, facts_c=LSTM_facts(LSTM_facts_inputs)\n",
    "    facts_states= [facts_h,facts_c]\n",
    "    \n",
    "    LSTM_rela_inputs = Input(shape=(None, relations_shape[2]))\n",
    "    LSTM_rela = LSTM(hidden_units,return_state=True)\n",
    "    LSTM_rela_output,rela_h,rela_c=LSTM_rela(LSTM_rela_inputs)\n",
    "    rela_states= [rela_h,rela_c]\n",
    "    \n",
    "    added_h = keras.layers.Add()([info_h,questions_h,facts_h,rela_h])\n",
    "    added_c = keras.layers.Add()([info_c,questions_c,facts_c,rela_c])\n",
    "    states = [added_h,added_c]\n",
    "    \n",
    "    decoder_inputs = Input(shape=(1, labels_index_len))\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_dense = Dense(labels_index_len, activation='softmax')\n",
    "\n",
    "    all_outputs = []\n",
    "    inputs = decoder_inputs\n",
    "    for _ in range(label_pad_up_to):\n",
    "        # Run the decoder on one timestep\n",
    "        outputs, state_h, state_c = decoder_lstm(inputs,\n",
    "                                             initial_state=states)\n",
    "        outputs = decoder_dense(outputs)\n",
    "        # Store the current prediction (we will concatenate all predictions later)\n",
    "        all_outputs.append(outputs)\n",
    "        # Reinject the outputs as inputs for the next loop iteration\n",
    "        # as well as update the states\n",
    "        inputs = outputs\n",
    "        states = [state_h, state_c]\n",
    "\n",
    "    # Concatenate all predictions\n",
    "    decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n",
    "\n",
    "    # Define and compile model as previously\n",
    "    model_T = Model([LSTM_info_inputs, LSTM_questions_inputs, LSTM_facts_inputs, \\\n",
    "                     LSTM_rela_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    model_T.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model_T\n",
    "\n",
    "#Example inputs: X1 = \"There are four apples at the top of the tree.\" X2 = \"What do you need to get the apples?\"\n",
    "# X3 = \"Trees are high.\"  Label result = \"Trees -> High\"\n",
    "#Example inputs: X1 = \"There are four apples at the top of the tree.\" X2 = \"What do you need to get the apples?\"\n",
    "# X3 = \"Pick apples from tree.\"  Label result = \"Apples -> On tree\"\n",
    "#Example inputs: X1 = \"Apples->On tree, Tree->High\" X2 = \"What do you need to get the apples?\"\n",
    "# X3 = \"You can go on tree with a ladder.\"  Label result = \"Ladder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "cascaded_reasoning loaded\n",
      "Train on 30 samples, validate on 4 samples\n",
      "Epoch 1/3\n",
      "30/30 [==============================] - 62s 2s/step - loss: 3.8728 - acc: 0.0033 - val_loss: 3.7941 - val_acc: 0.7800\n",
      "Epoch 2/3\n",
      "30/30 [==============================] - 1s 46ms/step - loss: 3.7204 - acc: 0.9080 - val_loss: 3.5629 - val_acc: 0.7800\n",
      "Epoch 3/3\n",
      "30/30 [==============================] - 1s 43ms/step - loss: 3.3305 - acc: 0.9080 - val_loss: 2.1994 - val_acc: 0.7800\n",
      "model fit\n"
     ]
    }
   ],
   "source": [
    "#tests.test_model_final(model_final)\n",
    "batch_size = 34\n",
    "\n",
    "print(len(label_index_to_words))\n",
    "\n",
    "cascaded_reasoning= cascaded_reasoning(vectorized_info.shape, vectorized_questions.shape, \\\n",
    "                       vectorized_fact_list.shape, vectorized_relations.shape, len(label_index_to_words))\n",
    "\n",
    "print('cascaded_reasoning loaded')\n",
    "\n",
    "# Prepare decoder input data that just contains the start character\n",
    "# Note that we could have made it a constant hard-coded in the model\n",
    "decoder_input_data = np.zeros((batch_size, 1, len(label_index_to_words)))\n",
    "decoder_input_data[:, 0, label_words_to_index['\\t']] = 1.\n",
    "\n",
    "cascaded_reasoning.fit([vectorized_info, vectorized_questions, \\\n",
    "                        vectorized_fact_list, vectorized_relations, decoder_input_data], \\\n",
    "                       label_array, batch_size=batch_size, epochs=3, validation_split=0.1)\n",
    "\n",
    "print(\"model fit\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hidden_units = 128\n",
    "latent_dim = 128 \n",
    "labels_index_len = len(label_index_to_words)\n",
    "    \n",
    "LSTM_info_inputs = Input(shape=(None, vectorized_info.shape[2]))\n",
    "LSTM_info = LSTM(hidden_units,return_state=True)\n",
    "LSTM_info_output, info_h, info_c=LSTM_info(LSTM_info_inputs)\n",
    "info_states = [info_h,info_c]\n",
    "    \n",
    "LSTM_questions_inputs = Input(shape=(None, vectorized_questions.shape[2]))\n",
    "LSTM_questions = LSTM(hidden_units,return_state=True)\n",
    "LSTM_questions_output, questions_h, questions_c=LSTM_questions(LSTM_questions_inputs)\n",
    "questions_states= [questions_h,questions_c]\n",
    "    \n",
    "LSTM_facts_inputs = Input(shape=(None, vectorized_fact_list.shape[2]))\n",
    "LSTM_facts = LSTM(hidden_units,return_state=True)\n",
    "LSTM_facts_output, facts_h, facts_c=LSTM_facts(LSTM_facts_inputs)\n",
    "facts_states= [facts_h,facts_c]\n",
    "    \n",
    "LSTM_rela_inputs = Input(shape=(None, vectorized_relations.shape[2]))\n",
    "LSTM_rela = LSTM(hidden_units,return_state=True)\n",
    "LSTM_rela_output,rela_h,rela_c=LSTM_rela(LSTM_rela_inputs)\n",
    "rela_states= [rela_h,rela_c]\n",
    "    \n",
    "added_h = keras.layers.Add()([info_h,questions_h,facts_h,rela_h])\n",
    "added_c = keras.layers.Add()([info_c,questions_c,facts_c,rela_c])\n",
    "states = [added_h,added_c]\n",
    "\n",
    "encoder_model = Model([LSTM_info_inputs,LSTM_questions_inputs,LSTM_facts_inputs,LSTM_rela_inputs],states)\n",
    "\n",
    "fact_model = Model(LSTM_facts_inputs, facts_states)\n",
    "ques_model = Model(LSTM_questions_inputs, questions_states)\n",
    "info_model = Model(LSTM_info_inputs, info_states)\n",
    "rela_model = Model(LSTM_rela_inputs, rela_states)\n",
    "\n",
    "decoder_inputs = Input(shape=(1, labels_index_len))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_dense = Dense(labels_index_len, activation='softmax')\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence(info_input, facts_input, ques_input, rela_input,label_words_to_index=label_words_to_index, \\\n",
    "                                        label_index_to_words=label_index_to_words, label_pad_up_to=label_pad_up_to):\n",
    "    \n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict([info_input, facts_input, ques_input, rela_input])\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, len(label_words_to_index)))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, label_words_to_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = label_index_to_words[sampled_token_index]\n",
    "        decoded_sentence += sampled_char + \" \"\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > label_pad_up_to):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, len(label_words_to_index)))\n",
    "        target_seq[0, 0, label_words_to_index['\\t']] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_info =  vectorized_info[0,:,:]\n",
    "test_info = test_info.reshape(1,*test_info.shape) \n",
    "test_fact = vectorized_fact_list[0,:,:]\n",
    "test_fact = test_fact.reshape(1,*test_fact.shape)\n",
    "test_ques = vectorized_questions[0,:,:]\n",
    "test_ques = test_ques.reshape(1,*test_ques.shape)\n",
    "test_rela = vectorized_relations[0,:,:]\n",
    "test_rela = test_rela.reshape(1,*test_rela.shape)\n",
    "\n",
    "decoded_sentence = decode_sequence(test_info, test_fact, test_ques, test_rela)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EACH EACH EACH 0 0 0 LEN LEN LEN LEN LEN LEN LEN LEN \n"
     ]
    }
   ],
   "source": [
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'relations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-2f9aeb51f6f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"VERB\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mindx_v\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mrelations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"V%d = %s\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindx_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mverb_key\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"V%d\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindx_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mreverse_verb_key\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"V%d\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindx_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'relations' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Train the final model\n",
    "\n",
    "#Example test:There are 4 doctors working in a clinic. Each doctor has 2 nurses assisting them. There\n",
    "#are two receptionists, Jay and Molly, working at the reception.\n",
    "#On Monday, 23 patients made appointments with each doctor. However, 6 of the\n",
    "#patients did not show up. How many patients visited the clinic on Monday?\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence = \"There are 4 doctors working in a clinic. There are 4 doctors working in a clinic. \\\n",
    "          Each doctor has 2 nurses assisting them. There are two receptionists, Jay and Molly, \\\n",
    "          working at the reception. On Monday, 23 patients made appointments with each doctor. \\\n",
    "          However, 6 of the patients did not show up.\"\n",
    "question = \"How many patients visited the clinic on Monday?\"\n",
    "doc = nlp(sentence)\n",
    "indx_n = 0\n",
    "indx_v = 0\n",
    "indx_num = 0\n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        indx_n += 1\n",
    "        relations.append(\"N%d = %s\".format(indx_n, token.text))\n",
    "        noun_key[token.text] = \"N%d\".format(indx_n)\n",
    "        reverse_noun_key[\"N%d\".format(indx_n)]=token.text\n",
    "    if token.pos_ == \"VERB\":\n",
    "        indx_v += 1\n",
    "        relations.append(\"V%d = %s\".format(indx_v, token.text))\n",
    "        verb_key[token.text] = \"V%d\".format(indx_v)\n",
    "        reverse_verb_key[\"V%d\".format(indx_v)]=token.text\n",
    "    if token.dep_ == \"num\" or token.dep_ == \"num\": #this is deprecated\n",
    "        indx_num += 1\n",
    "        relations.append(\"NUM%d = %s\".format(indx_num, token.text))\n",
    "        number_key[token.text] = \"NUM%d\".format(indx_num)\n",
    "        reverse_number_key[\"NUM%d\".format(indx_num)]=token.text\n",
    "\n",
    "#We will not convert the numbers. Instead, we will use vector representations of the numbers. The machine can figure \n",
    "#out the relation between the numbers in the sentence. We will represent every word with a 10-digit vector. The \n",
    "#first digits will indicate the type of word; for example: representative can be represented by the vector 9000000602,  \n",
    "#where 9 is for noun. Another example: work = 6000000052, where 6 is for verb. Now, for numbers, there will be zeros in the \n",
    "#beginning. For example, 3 will simply be 0000000003, and 24 = 0000000024. We will let the network figure out that these \n",
    "#represent actual numbers.   \n",
    "\n",
    "#create variables to be used in assignments:\n",
    "var_list = []\n",
    "for indx_var in range(10):\n",
    "    var_list[indx_var+1] = \"VAR%d\".format(indx_var+1) #Add 1 so that VAR list starts with VAR1\n",
    "    \n",
    "fact_list[0] = \"A physician, medical practitioner, medical doctor, or simply doctor is a professional who practises medicine, \\\n",
    "which is concerned with promoting, maintaining, or restoring health through the study, diagnosis, and treatment \\\n",
    "of disease, injury, and other physical and mental impairments.\" \n",
    "\n",
    "#Now collect info about each of the nouns based on the question:\n",
    "for noun in noun_list:\n",
    "    #example: noun being focussed = doctors \n",
    "    for fact in fact_list:  \n",
    "        inputs = [sentence, question, relations, facts]\n",
    "        #we expect an output that will yield a sort of relation between the words that will be useful to answer the question:\n",
    "        #these relations will be initially input by humans to train the network on what to look for\n",
    "        #in this case, the output could be a list of four or five relations, each relation composed of up to four words: \n",
    "        #[doctor->practices->medicine, medicine->about->health, 23 patients->made->appointments, 4->doctors]\n",
    "        #The output from this iteration will be ammended to the relations list and fed into the next iteration.\n",
    "        #It is possible to have the network find the relations itself by stacking several of these networks. However, the\n",
    "        #resulting mega-network will be too huge to train. A supercomputer may be necessary.   \n",
    "        #For now, we will fix the question and info, and iterate over the fact sentences. Labels will correspond to facts. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
